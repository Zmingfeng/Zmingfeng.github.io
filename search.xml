<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据结构的基本概念]]></title>
    <url>%2F2018%2F08%2F25%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[本篇内容旨在对数据结构的基本概念及术语进行介绍，并从全局的角度对数据结构有一个大体的认识。 数据结构的基本术语数据的组成数据：信息的载体，是描述客观事物属性的数、字符以及所有能输入到计算机中并被计算机程序识别和处理的符号的集合。 数据项：构成数据元素不可分割的最小单位。 数据元素：数据的基本单位，通常作为一个整体，一个个的数据元素就组成了数据（一个数据元素含有多个数据项）。 数据对象：数据的一个子集，它是同一类数据元素的集合。 例：一个班级可以看作是数据对象，它由一个个学生组成，学生记录可以看作是一类数据元素，学生的学号、姓名、性别可以看成是数据项。 数据类型原子类型：其值不可分割的数据类型； 结构类型：其值可以分成若干个结构类型或者原子类型的数据类型； 抽象数据类型：是指一个数学模型以及定义在模型上的一组操作，其定义仅仅取决于其逻辑特性，而与计算机的内部实现无关。 数据结构的组成数据结构是相互之间存在一种或多种特定关系的数据元素的集合，其包括三个方面：逻辑结构、存储结构以及数据的运算。 逻辑结构：数据元素之间的逻辑关系，独立于计算机，跟数据存储无关。 存储结构：数据结构在计算机中的表示，包括数据元素的表示和关系的表示，存储结构是逻辑结构用计算机语言的实现，依赖于计算机语言。 数据的运算：运算的定义是针对逻辑结构的，指出了运算的功能；运算的实现是针对存储结构的，指出了运算的具体步骤。 逻辑结构逻辑结构分为：线性结构和非线性结构。 线性结构：结构中的数据元素之间只存在一对一的关系，一般线性表、栈和队列、串和数据以及广义表都属于线性结构。 非线性结构：结构之中的数据元素之间存在一对多（树）或多对多（图）的关系，树和图属于非线性结构。 集合是一种特殊的逻辑结构，内部数据元素之间除了“同属于一个集合”外没有任何关系，可以被分为非线性结构之中。 存储结构存储结构可以分为：顺序存储、链式存储、索引存储以及散列存储。 顺序存储：把逻辑上相邻的元素存储在物理上也相邻的存储单元中。特点是可以随机存取，每个元素占用的存储空间少；缺点是只能用相邻的整块存储单元，易产生外部碎片。 链式存储：逻辑上相邻的元素在物理上不一定相邻，借助存储地址进行访问。优点是不会产生外部碎片；缺点是元素因存储指针会占用额外空间，并且只能顺序存储。 索引存储：存储元素信息的同时还要建立附加的索引表（由一个个的索引项（形如 关键字，地址）组成）。优点是检索速度快；缺点是增加了索引表，占用额外空间，增删数据时要同时增删索引项。 散列（Hash）存储：通过元素的关键字计算出元素的存储地址。优点是查找、增加及删除元素都很快；缺点是散列函数不好会导致元素存储单元的冲突，事实上任何散列函数都会出现元素存储单元冲突的可能，而解决冲突会增加时间和空间开销。 算法的特性、要求及度量算法是对特定问题求解步骤的一种描述 特性算法的基本特性：有穷性、确定性、可行性、输入及输出。 要求算法的要求（目标）：正确性、可读性、健壮性、效率（算法执行的时间）高、存储量（算法执行过程中所需要的最大存储空间）需求低。 度量算法的度量分为：时间复杂性和空间复杂性。 时间复杂性：算法中所有语句的频度（一条语句在算法中被执行的次数）之和称为T(n)，被称为算法问题规模n的函数，时间复杂度即是分析T(n)的数量级。 由于在最深层循环的语句的频度与T(n)同数量级，所以一般通过最深层循环语句的频度f(n)（渐近时间复杂度）来表示时间复杂度。即 T(n) = O(f(n))，”O“为计算数量级。 算法的时间复杂度不仅依赖于问题的规模，也依赖于待输入元素的性质（比如输入元素的初始状态）。 常见的渐近时间复杂度：O(1) &lt; O(log2n) &lt; O(n) &lt; O(nlog2n) &lt; O(n^2) &lt; O(n^3) &lt; O(2^n) &lt; O(n!) &lt; O(n^n) 空间复杂性：算法所耗费的存储空间，也是算法问题规模的函数，记为O(g(n))； 上级程序除了需要存储空间存放本身执行指令、数据外，还需要辅助空间来实现计算及进行操作。 若是输入数据所占空间仅取决于问题本身，和算法无关，就只需要分析除输入和程序之外的额外空间即可。 算法原地工作：算法所需额外空间是常量，即O(1)。 数据结构的组织逻辑图：]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析C++中的引用]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%B5%85%E6%9E%90C-%E4%B8%AD%E7%9A%84%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[为已定义变量创建别名定义变量时，在变量前加上‘&amp;’符号，就表示定义一个引用。首先声明，引用只是为引用对象创建了别名，而并未开辟新的内存。先看下面一段代码：123456double i = 42.0;double &amp;r = i;std::cout &lt;&lt; "the value of i is " &lt;&lt; i &lt;&lt; std::endl;std::cout &lt;&lt; "the value of r is " &lt;&lt; r &lt;&lt; std::endl;std::cout &lt;&lt; "the address of i is "&lt;&lt; &amp;i &lt;&lt; std::endl;std::cout &lt;&lt; "the address of r is " &lt;&lt; &amp;r &lt;&lt; std::endl; 定义一个double类型的变量，并对其进行同类型引用，输出如下：1234the value of i is 42the value of r is 42the address of i is 0x7ffcf8119c88the address of r is 0x7ffcf8119c88 可以看到，不仅指向的字面值相等，其内存地址也相等。 非const引用的限制非const引用只能进行同类型引用，否则编译器会报错比如：12double i = 42.0;int &amp;r = i; 运行程序，会报错：1234In function ‘int main()’:error: invalid initialization of non-const reference of type ‘int&amp;’ from an rvalue of type ‘int’ int &amp;r = i; ^ 其实这样也无可厚非，毕竟非const引用，可以改变引用对象，而不同类型的修改，会产生无法预料的错误。 同理，非const引用也不可用于右值初始化，因为字面值常量是不可更改的。 const引用值得注意的细节const引用既可用于同类型，也可用于不同类型，也可用于字面值常量。在const引用应用于不同类型的引用时会创建中间变量，也就是说该引用并非是指向你想要指向的引用对象了。示例如下:123456double i = 42.0;const int &amp;r = i;std::cout &lt;&lt; "the value of i is " &lt;&lt; i &lt;&lt; std::endl;std::cout &lt;&lt; "the value of r is " &lt;&lt; r &lt;&lt; std::endl;std::cout &lt;&lt; "the address of i is "&lt;&lt; &amp;i &lt;&lt; std::endl;std::cout &lt;&lt; "the address of r is " &lt;&lt; &amp;r &lt;&lt; std::endl; 最终会输出不同的内存地址：1234the value of i is 42the value of r is 42the address of i is 0x7ffc1a38a528the address of r is 0x7ffc1a38a524 也就是说，其实在引用时进行了如下操作：123double i = 42.0;int temp = i;const int &amp;r = temp; 补充虽然不能对变量的const引用，不能通过引用改变被引用对象的内容，但却可以通过被引用对象自己来修改其值。如下：1234567double i = 42.0;const double &amp;r = i;i = 50.0;std::cout &lt;&lt; "the value of i is " &lt;&lt; i &lt;&lt; std::endl;std::cout &lt;&lt; "the value of r is " &lt;&lt; r &lt;&lt; std::endl;std::cout &lt;&lt; "the address of i is "&lt;&lt; &amp;i &lt;&lt; std::endl;std::cout &lt;&lt; "the address of r is " &lt;&lt; &amp;r &lt;&lt; std::endl; 输出结果如下：1234the value of i is 50the value of r is 50the address of i is 0x7fff7d83b988the address of r is 0x7fff7d83b988 可以看到通过被引用对象将两者的内容都改变了。 对引用的一些分析就谈到这里咯，如有不对，还望指正！]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3编写猜数字小游戏]]></title>
    <url>%2F2018%2F03%2F16%2FPython3%E7%BC%96%E5%86%99%E7%8C%9C%E6%95%B0%E5%AD%97%E5%B0%8F%E6%B8%B8%E6%88%8F%2F</url>
    <content type="text"><![CDATA[生成神秘数字序列导入random模块1import random 使用range()生成0-9的序列，并使用random模块将其随机化1234567def getSecretNum(): numbers = list(range(10)) random.shuffle(numbers) secretNum = '' for i in range(NUM_LENGTH): secretNum += str(numbers[i]) return secretNum random模块的shuffle方法可将输入的原序列替换成随机序列。 对玩家输入的序列进行处理根据玩家输入的序列给出相应的提示123456789101112131415def getClues(guess,secretNum): # return a string made up of Pico,Fermi,Bagels if guess == secretNum: return "Great,you got it!" else: clues = [] for i in range(len(guess)): if guess[i] == secretNum[i]: clues.append('Fermi') elif guess[i] in secretNum: clues.append('Pico') if len(clues) == 0: return 'Bagels' clues.sort() return ' '.join(clues) 定义一个参数检查函数，避免出现不正当输入123456789def isOnlyDigits(num): #judge whether the input from gameplayer is true or not if num == '': return False; else: for i in num: if i not in '0 1 2 3 4 5 6 7 8 9'.split(): return False return True 源码：Begals.py]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取需要验证码的课程表]]></title>
    <url>%2F2018%2F03%2F09%2FGetClassTable%2F</url>
    <content type="text"><![CDATA[首先声明，本次爬取的是华北科技学院的课程表，在其他学校不一定适用。 基础知识补充1.requestsrequests是一个强大的第三方库，可以很大的简化抓取网页的步骤，只要传递几个参数，就能抓取到期望的页面。具体使用，可以参考:Requests快速上手 2.Beautiful SoupBeautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.简单来说，Beautiful Soup是个第三方的网页解析库，具体使用方法，参考:Beautiful Soup 4.4.0 文档 3.cookies指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密），一般第一次访问某个站点，服务器就会返回cookies，以便记录此次会话，而用户下次访问这些站点，就要带着这个cookies访问。 抓取课程表所在的网页这一步是为后面的工作打下基础，首先将自己拦截的浏览器headers组成dict，然后带着这些headers抓取课程表的初始网页，获取此次会话的cookies。123456789101112import requestsurl0 = 'http://jwgl.ncist.edu.cn/ZNPK/KBFB_ClassSel.aspx'header = &#123;'Accept': '*/*','Accept-Encoding': 'gzip, deflate','Accept-Language': 'zh-Hans-CN,zh-Hans;q=0.5','Host': 'jwgl.ncist.edu.cn','Referer': 'http://jwgl.ncist.edu.cn/ZNPK/KBFB_ClassSel.aspx','User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 10.0; WOW64; \Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3)'&#125;r = requests.get(url0,headers=header) 获取网页中的有效信息上文提到了，用户下次访问站点，需要带着cookies，那么怎么分辨cookies？ 这个用浏览器的开发者工具或者获取抓包工具，比如fidder，都能查看浏览器访问时，都携带了哪些内容，像上文的请求headers就是通过抓包工具，分析得到。在此次的访问中，可以获取到cookies，以及一些行政班级的信息，将其记录下来，后面会用到。1234cookies = cookies['Set-Cookie'].split('=')cookies = &#123;cookies[0]:cookies[1].split(';')[0]&#125;Sel_XZBJ_l = BeautifulSoup(r.text,'lxml').find('select',&#123;'name':'Sel_XZBJ'&#125;).find_all('option')Sel_XNXQ_l = BeautifulSoup(r.text,'lxml').find('select',&#123;'name':'Sel_XNXQ'&#125;).find_all('option') 交互式输入post头想要获取课程表，需要提交一些信息，比如，哪个班，哪个学期等，至于需要哪些信息才能访问，都是通过抓包工具拦截分析得到，甚至访问的步骤都是通过抓包工具分析得到。12345678910111213141516171819202122232425262728293031323334353637383940while True: Sel_XNXQ_Temp = input('请输入学年学期：（例如：2016-2017学年第二学期）') for XNXQ_temp in Sel_XNXQ_l: if XNXQ_temp.text == Sel_XNXQ_Temp: Sel_XNXQ = XNXQ_temp['value'] break else: Sel_XNXQ = '' if Sel_XNXQ != '': break else: print('输入错误！')while True: type = input('以何种格式打印（1/2）：') if type == '1' or type == '2': break else: print('输入错误！')while True: Sel_XZBJ_Temp = input('请输入班级（例如：自卓B141）：') for class_temp in Sel_XZBJ_l: if class_temp.text.strip() == Sel_XZBJ_Temp: Sel_XZBJ = class_temp['value'] break else: Sel_XZBJ = '' if Sel_XZBJ != '': break else: print('未找到该班级，请重新输入！')while True: txtxzbj = input('是否包含公共人选课程（y/n）:') if txtxzbj == 'n' or txtxzbj == 'N': txtxzbj = '1' break elif txtxzbj == 'y' or txtxzbj == 'Y' or txtxzbj == '': txtxzbj = '' break else: print('输入错误！') 获取验证码，并人工识别有的网站，想要跳到另一个页面需要验证码，而有的是图片验证码，在网页源码里并没有。这个时候通过抓包工具，看一下点击验证码更换图片时，浏览器都做了什么。本文中浏览器是向某个网页提交了一个get，再由服务器返回一个图片网页，这样我就能获取到这个图片验证码了,然后通过交互模式，人工输入验证码，就可以通过验证。12345url1 = 'http://jwgl.ncist.edu.cn/sys/ValidateCode.aspx'r = requests.get(url1,headers=header,cookies=cookies)image = Image.open(BytesIO(r.content))image.show()yzm = input('请输入图片验证码：') post方法提交验证码等信息1234567891011header['Accept'] = 'image/gif, image/jpeg, image/pjpeg, application/x-ms-application, application/xaml+xml, \application/x-ms-xbap, application/vnd.ms-excel, application/vnd.ms-powerpoint, application/msword, */*'url = 'http://jwgl.ncist.edu.cn/ZNPK/KBFB_ClassSel_rpt.aspx'postData = &#123;'Sel_XNXQ':Sel_XNXQ,'txtxzbj':txtxzbj,'Sel_XZBJ':Sel_XZBJ,'type':type,'txt_yzm': yzm&#125;r = requests.post(url,data=postData,headers=header,cookies=cookies) 分析post返回的网页通过分析post返回的网页，可以得到课程表的图片地址，接着get就行了。12345678910x = BeautifulSoup(r.text,'lxml').find('img')header['Accept'] = '*/*'header['Referer'] = 'http://jwgl.ncist.edu.cn/ZNPK/KBFB_ClassSel_rpt.aspx'url = 'http://jwgl.ncist.edu.cn/ZNPK/' + x['src']r = requests.get(url,headers=header,cookies=cookies)jpg = Image.open(BytesIO(r.content))jpg.show()judge = input('是否保存图片（y/n）')if judge == 'Y' or judge == 'y': jpg.save("./out.jpg", "jpeg") 至此，课程表的获取工作已经完成。 注意：图片是二进制文件。 源码：GetClassTable.py]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取百度天气预报并通过邮件通知]]></title>
    <url>%2F2018%2F03%2F09%2FMyGetWeather%2F</url>
    <content type="text"><![CDATA[requests+BeautifulSoup抓取百度天气上的天气预报python的优势在于有着各种各样功能强大的标准库和第三方库，使得十分简洁的python代码便可以实现相对复杂的功能，首先先了解以下利用requests抓取网页 使用requests模块抓取网页1.导入requests库1import requests 2.抓取百度天气网页源码12url = 'http://www.baidutianqi.com/'html = requests.get(url).content 抓取到的网页是bytes型，想要得到字符型，就像下面这样把content换成text就行：12url = 'http://www.baidutianqi.com/'html = requests.get(url).text requests的用法很简单，可以参考：requests官方文档 利用BeautifulSoup解析网页获取天气信息1.导入BeautifulSoup库1from bs4 import BeautifulSoup 2.利用BeautifulSoup解析网页12tag_tmp = BeautifulSoup(html,'lxml')provinceInfo = tag_tmp.find(id='provinces').find('ul').find_all('li') ‘lxml’是lxml HTML解析器，速度快，容错能力强。将html参数传入BeautifulSoup类中，获得一个BeautifulSoup对象，其实就是一个tag对象，可调用对象find()和find_all()方法来匹配到期望的标签。 注意：find()方法返回的是文档中符合条件的tag对象，而find_all()返回的是符合条件的所有的tag组成的list12cityName = []cityLink = [] 建立两个list，存放城市的名称和城市天气的链接123456789101112weather = &#123;&#125;for x in provinceInfo: html = requests.get(x.a['href']).content tag_tmp = BeautifulSoup(html, 'lxml') city_tmp = tag_tmp.find(id='citys').find('ul').find_all('li') for y in city_tmp: tmp = y.find('a') cityName.append(tmp.get_text().strip('天气预报')) cityLink.append(tmp['href']) result = list(zip(cityName,cityLink)) for x,y in result: weather[x] = y a[‘href’]是获取a标签中的’href’属性，strip()不传入参数时，默认去掉字符串前后的空格，zip()可以将传入的两个Iterable对应的元素组成tuple，weather是存放城市和天气对应关系的dict，这样城市和天气信息就对应起来了。 想了解BeautifulSoup的具体用法，参考：BeautifulSoup官方文档3.对获取的城市天气所在链接进行处理123456789101112cityIn = '上海' if cityIn in weather.keys(): html = requests.get(weather[cityIn]).content weather_tmp = BeautifulSoup(html, 'lxml') weather7days = weather_tmp.find(id="weather").find('ul').find_all('li') day = weather7days[0] l_tmp = day.get_text().split(' ') w = l_tmp[0].strip() x = l_tmp[2].strip() + l_tmp[3].strip() y = x.split('℃')[0].strip() + '℃' z = x.split('℃')[1].strip() print(w+'\n'+y+'\n'+z + '\n' + '-'*20) weather7days字存放的是连续七天的天气预报的tag组成的list，用day = weather7days[0]获取今天天气预报的tag，day.get_text()获取tag中天气文本（温度，风级，阴晴等），接下来就是文本分离部分，不同网站的天气格式略有不同，可以根据不同的网站，使用不同的分离方法。 使用smtplib+email发送邮件email模块用于生成邮件，而smtplib模块用于发送邮件，具体代码如下：1234567891011121314151617181920212223from email.mime.text import MIMETextimport smtplibdef send_email(content): mail_to = "目标邮箱" mail_server = "源邮箱服务器" mail_user = "源邮箱" mail_pass = "源邮箱密码" msg = MIMEText(content) msg['Subject'] = '今日天气预报' msg['From'] = mail_user msg['To'] = mail_to try: s = smtplib.SMTP() s.connect(mail_server) s.login(mail_user, mail_pass) s.sendmail(mail_user, mail_to, msg.as_string()) s.close() print('发送成功！') return True except Exception as e: print('发送失败！') print(e) return False 首先导入了email和stmplib库，然后定义了一个sned_email()函数，mail_to代表目标邮箱，mail_server是你的邮箱服务器，例如163邮箱是”smtp.163.com”，mail_user和mail_pass是你的邮箱和密码。 源码：MyGetWeather.py]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取四六级成绩查询系统]]></title>
    <url>%2F2018%2F03%2F09%2FGet46Score%2F</url>
    <content type="text"><![CDATA[使用requests获取网页源码1.分析浏览器访问网页的过程 首先选择 http://www.chsi.com.cn/cet/ 进行四六级的查询，查看浏览器提交的表单，然后发现，实际网址就是 http://www.chsi.com.cn/cet/query 后面加上输入的准考证号，和姓名，故选择 http://www.chsi.com.cn/cet/query 作为访问基址。 2.查看46级成绩所在页面的源码12345678import requestsurlBase = 'http://www.chsi.com.cn/cet/query'zkzh = '***************'xm = '***'payload = &#123;'zkzh':zkzh,'xm':xm&#125;html = requests.get(urlBase,params=payload)print(html.url) “*“替换成你的准考证号和姓名，requests.get()返回的对象的url属性，保存的是当前访问的网页的链接，但是事实上我们并没有得到下面这样的链接，这是因为没有添加一些header。1http://www.chsi.com.cn/cet/query？zkzh=xiaoming&amp;xm=123 3.添加header，伪装成浏览器将最后两行代码改成下面这样：123456...headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Referer': 'http://www.chsi.com.cn/cet/' &#125;html = requests.get(urlBase,params=payload,headers=headers) print(html.url) headers是个dict，里面的headers映射关系，可以通过fidder监控网页访问得到。 4.获取网页源码12...data = html.content content属性代表返回的是bytes型的网页文件。 使用BeautifulSoup进行网页解析123456789...tag_tmp = BeautifulSoup(data, 'lxml')m = tag_tmp.find(id='leftH').find('table')y = m.find_all('tr')for x in y: if x.find('td') != None: print(x.find('th').get_text().strip() + x.find('td').get_text().strip()) else: print(' ' * 3 + x.find('th').get_text().strip()) 将获取的四六级成绩所在的页面源码提交给BeautifulSoup进行网页解析，分析得到的信息再通过一定的格式组合，就可以打印出来查询的个人信息和四六级成绩了。 源码：Get46Score.py]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量爬取豆瓣上的图片]]></title>
    <url>%2F2018%2F03%2F09%2FDownloadImag%2F</url>
    <content type="text"><![CDATA[利用Python3的urllib.request模块实现静态网页抓取urllib.request模块定义了很多的函数和类来帮助Programmers快速高效的处理URL。 首先，抓取豆瓣的首页：1234567# coding : utf-8import urllib.requesturl = 'http://www.douban.com'#获取静态网页webpage = urllib.request.urlopen(url)data = webpage.read().decode('utf-8')print(data) urllib.request.urlopen()函数返回的是http.client.HTTPResponse的实例，通过调用http.client.HTTPResponse类下的read()方法来获取服务器应答的bytes格式的页面，通过decode()方法进行解码，获得utf-8格式的html源码。 伪装成浏览器访问爬虫并不是任何时刻都可以成功的，很多网站都有防爬虫机制，拒绝非浏览器的访问，这种情况下，可以通过添加一些header来伪装成浏览器，以欺骗站点的检查：12345678910# -*- coding:utf-8 -*-import urllib.requesturl = 'http://www.douban.com'header = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \ (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36' &#125;req = urllib.request.Request(url = url,headers = header)data = urllib.request.urlopen(req).read().decode()print(data) urllib.request.Request类返回一个URL request的抽象，然后调用urllib.request.urlopen()函数，传入urllib.request.Request处理后的请求。url header可以通过fidder监控访问站点时浏览器向网站发送的请求报文中获取。 利用正则表达式提取出网页中的图片链接正则表达式的相关知识，参考: 正则表达式指南 代码如下：12345678910111213141516# -*- coding:utf-8 -*-import urllib.request,re,os,timeif __name__ == '__main__': url = 'http://www.douban.com' header = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \ (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6', 'Accept-Encoding': 'sdch' &#125; req = urllib.request.Request(url = url,headers = header) contentBytes = urllib.request.urlopen(req).read().decode() for link,t in set(re.findall(r'(https://[^\s]*?(jpg|png|gif))',contentBytes)): time.sleep(1) print(link) 发散式爬取图片选择豆瓣作为入口点，使用set存放url可以有效的防止url被重复访问，降低效率：12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-import urllib.request,re,os,timetargetDir = r'e:\Git\Crawler\html_img'def Link2File_Path(path): if not os.path.isdir(targetDir): os.mkdir(targetDir) index = path.rindex('/') + 1 file_path = os.path.join(targetDir,path[index:]) return file_pathif __name__ == '__main__': url_crawled = set([]) url_crawlering = &#123;'http://www.douban.com'&#125; header = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \ (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6', 'Accept-Encoding': 'sdch' &#125; while len(url_crawlering): url1 = url_crawlering.pop() if url1 not in url_crawled: url_crawled.add(url1) req = urllib.request.Request(url = url1,headers = header) contentBytes = urllib.request.urlopen(req).read() for url2 in set(re.findall(r'(https://[^\s]*?com)',str(contentBytes))): url_crawlering.add(url2) for link,t in set(re.findall(r'(https://[^\s]*?(jpg|png|gif))',str(contentBytes))): time.sleep(1) print(link) try: urllib.request.urlretrieve(link,Link2File_Path(link)) #Copy a network object denoted by a URL to a local file except: print('Download image failed') 源码：DownloadImag.py]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
